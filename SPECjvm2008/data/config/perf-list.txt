  branch-instructions OR branches                    [Hardware event]
  branch-misses                                      [Hardware event]
  bus-cycles                                         [Hardware event]
  cache-misses                                       [Hardware event]
  cache-references                                   [Hardware event]
  cpu-cycles OR cycles                               [Hardware event]
  instructions                                       [Hardware event]
  ref-cycles                                         [Hardware event]
  alignment-faults                                   [Software event]
  bpf-output                                         [Software event]
  cgroup-switches                                    [Software event]
  context-switches OR cs                             [Software event]
  cpu-clock                                          [Software event]
  cpu-migrations OR migrations                       [Software event]
  dummy                                              [Software event]
  emulation-faults                                   [Software event]
  major-faults                                       [Software event]
  minor-faults                                       [Software event]
  page-faults OR faults                              [Software event]
  task-clock                                         [Software event]

tool:
  duration_time
  user_time
  system_time

cache:
  L1-dcache-loads OR cpu/L1-dcache-loads/
  L1-dcache-load-misses OR cpu/L1-dcache-load-misses/
  L1-dcache-stores OR cpu/L1-dcache-stores/
  L1-icache-load-misses OR cpu/L1-icache-load-misses/
  LLC-loads OR cpu/LLC-loads/
  LLC-load-misses OR cpu/LLC-load-misses/
  LLC-stores OR cpu/LLC-stores/
  LLC-store-misses OR cpu/LLC-store-misses/
  dTLB-loads OR cpu/dTLB-loads/
  dTLB-load-misses OR cpu/dTLB-load-misses/
  dTLB-stores OR cpu/dTLB-stores/
  dTLB-store-misses OR cpu/dTLB-store-misses/
  iTLB-load-misses OR cpu/iTLB-load-misses/
  branch-loads OR cpu/branch-loads/
  branch-load-misses OR cpu/branch-load-misses/
  node-loads OR cpu/node-loads/
  node-load-misses OR cpu/node-load-misses/
  node-stores OR cpu/node-stores/
  node-store-misses OR cpu/node-store-misses/
  branch-instructions OR cpu/branch-instructions/    [Kernel PMU event]
  branch-misses OR cpu/branch-misses/                [Kernel PMU event]
  bus-cycles OR cpu/bus-cycles/                      [Kernel PMU event]
  cache-misses OR cpu/cache-misses/                  [Kernel PMU event]
  cache-references OR cpu/cache-references/          [Kernel PMU event]
  cpu-cycles OR cpu/cpu-cycles/                      [Kernel PMU event]
  instructions OR cpu/instructions/                  [Kernel PMU event]
  mem-loads OR cpu/mem-loads/                        [Kernel PMU event]
  mem-stores OR cpu/mem-stores/                      [Kernel PMU event]
  ref-cycles OR cpu/ref-cycles/                      [Kernel PMU event]
  slots OR cpu/slots/                                [Kernel PMU event]
  topdown-bad-spec OR cpu/topdown-bad-spec/          [Kernel PMU event]
  topdown-be-bound OR cpu/topdown-be-bound/          [Kernel PMU event]
  topdown-fe-bound OR cpu/topdown-fe-bound/          [Kernel PMU event]
  topdown-retiring OR cpu/topdown-retiring/          [Kernel PMU event]
  cstate_core/c6-residency/                          [Kernel PMU event]
  cstate_core/c7-residency/                          [Kernel PMU event]
  cstate_pkg/c10-residency/                          [Kernel PMU event]
  cstate_pkg/c2-residency/                           [Kernel PMU event]
  cstate_pkg/c3-residency/                           [Kernel PMU event]
  cstate_pkg/c6-residency/                           [Kernel PMU event]
  cstate_pkg/c7-residency/                           [Kernel PMU event]
  cstate_pkg/c8-residency/                           [Kernel PMU event]
  cstate_pkg/c9-residency/                           [Kernel PMU event]
  i915/actual-frequency/                             [Kernel PMU event]
  i915/bcs0-busy/                                    [Kernel PMU event]
  i915/bcs0-sema/                                    [Kernel PMU event]
  i915/bcs0-wait/                                    [Kernel PMU event]
  i915/interrupts/                                   [Kernel PMU event]
  i915/rc6-residency/                                [Kernel PMU event]
  i915/rcs0-busy/                                    [Kernel PMU event]
  i915/rcs0-sema/                                    [Kernel PMU event]
  i915/rcs0-wait/                                    [Kernel PMU event]
  i915/requested-frequency/                          [Kernel PMU event]
  i915/software-gt-awake-time/                       [Kernel PMU event]
  i915/vcs0-busy/                                    [Kernel PMU event]
  i915/vcs0-sema/                                    [Kernel PMU event]
  i915/vcs0-wait/                                    [Kernel PMU event]
  i915/vcs1-busy/                                    [Kernel PMU event]
  i915/vcs1-sema/                                    [Kernel PMU event]
  i915/vcs1-wait/                                    [Kernel PMU event]
  i915/vecs0-busy/                                   [Kernel PMU event]
  i915/vecs0-sema/                                   [Kernel PMU event]
  i915/vecs0-wait/                                   [Kernel PMU event]
  intel_bts//                                        [Kernel PMU event]
  intel_pt//                                         [Kernel PMU event]
  msr/aperf/                                         [Kernel PMU event]
  msr/cpu_thermal_margin/                            [Kernel PMU event]
  msr/mperf/                                         [Kernel PMU event]
  msr/pperf/                                         [Kernel PMU event]
  msr/smi/                                           [Kernel PMU event]
  msr/tsc/                                           [Kernel PMU event]
  power/energy-cores/                                [Kernel PMU event]
  power/energy-gpu/                                  [Kernel PMU event]
  power/energy-pkg/                                  [Kernel PMU event]
  power/energy-psys/                                 [Kernel PMU event]
  uncore_clock/clockticks/                           [Kernel PMU event]
  uncore_imc_free_running/data_read/                 [Kernel PMU event]
  uncore_imc_free_running/data_total/                [Kernel PMU event]
  uncore_imc_free_running/data_write/                [Kernel PMU event]

cache:
  l1d.replacement
       [Counts the number of cache lines replaced in L1 data cache]
  l1d_pend_miss.fb_full
       [Number of cycles a demand request has waited due to L1D Fill Buffer
        (FB) unavailability]
  l1d_pend_miss.fb_full_periods
       [Number of phases a demand request has waited due to L1D Fill Buffer
        (FB) unavailability]
  l1d_pend_miss.l2_stall
       [Number of cycles a demand request has waited due to L1D due to lack of
        L2 resources]
  l1d_pend_miss.pending
       [Number of L1D misses that are outstanding]
  l1d_pend_miss.pending_cycles
       [Cycles with L1D load Misses outstanding]
  l2_lines_in.all
       [L2 cache lines filling L2]
  l2_lines_out.non_silent
       [Modified cache lines that are evicted by L2 cache when triggered by an
        L2 cache fill]
  l2_lines_out.silent
       [Non-modified cache lines that are silently dropped by L2 cache when
        triggered by an L2 cache fill]
  l2_rqsts.all_code_rd
       [L2 code requests]
  l2_rqsts.all_demand_data_rd
       [Demand Data Read access L2 cache]
  l2_rqsts.all_rfo
       [RFO requests to L2 cache]
  l2_rqsts.code_rd_hit
       [L2 cache hits when fetching instructions, code reads]
  l2_rqsts.code_rd_miss
       [L2 cache misses when fetching instructions]
  l2_rqsts.demand_data_rd_hit
       [Demand Data Read requests that hit L2 cache]
  l2_rqsts.demand_data_rd_miss
       [Demand Data Read miss L2 cache]
  l2_rqsts.miss
       [Read requests with true-miss in L2 cache]
  l2_rqsts.references
       [All accesses to L2 cache]
  l2_rqsts.rfo_hit
       [RFO requests that hit L2 cache]
  l2_rqsts.rfo_miss
       [RFO requests that miss L2 cache]
  l2_rqsts.swpf_hit
       [SW prefetch requests that hit L2 cache]
  l2_rqsts.swpf_miss
       [SW prefetch requests that miss L2 cache]
  l2_trans.l2_wb
       [L2 writebacks that access L2 cache]
  lock_cycles.cache_lock_duration
       [Cycles when L1D is locked]
  longest_lat_cache.miss
       [Core-originated cacheable requests that missed L3 (Except hardware
        prefetches to the L3)]
  mem_inst_retired.all_loads
       [Retired load instructions Supports address when precise (Precise
        event)]
  mem_inst_retired.all_stores
       [Retired store instructions Supports address when precise (Precise
        event)]
  mem_inst_retired.any
       [All retired memory instructions Supports address when precise (Precise
        event)]
  mem_inst_retired.lock_loads
       [Retired load instructions with locked access Supports address when
        precise (Precise event)]
  mem_inst_retired.split_loads
       [Retired load instructions that split across a cacheline boundary
        Supports address when precise (Precise event)]
  mem_inst_retired.split_stores
       [Retired store instructions that split across a cacheline boundary
        Supports address when precise (Precise event)]
  mem_inst_retired.stlb_miss_loads
       [Retired load instructions that miss the STLB Supports address when
        precise (Precise event)]
  mem_inst_retired.stlb_miss_stores
       [Retired store instructions that miss the STLB Supports address when
        precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_fwd
       [Snoop hit a modified(HITM) or clean line(HIT_W_FWD) in another on-pkg
        core which forwarded the data back due to a retired load instruction
        Supports address when precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_miss
       [Retired load instructions whose data sources were L3 hit and
        cross-core snoop missed in on-pkg core cache Supports address when
        precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_no_fwd
       [Snoop hit without forwarding in another on-pkg core due to a retired
        load instruction, data was supplied by the L3 Supports address when
        precise (Precise event)]
  mem_load_l3_hit_retired.xsnp_none
       [Retired load instructions whose data sources were hits in L3 without
        snoops required Supports address when precise (Precise event)]
  mem_load_misc_retired.uc
       [Retired instructions with at least 1 uncacheable load or lock Supports
        address when precise (Precise event)]
  mem_load_retired.fb_hit
       [Number of completed demand load requests that missed the L1, but hit
        the FB(fill buffer), because a preceding miss to the same cacheline
        initiated the line to be brought into L1, but data is not yet ready in
        L1 Supports address when precise (Precise event)]
  mem_load_retired.l1_hit
       [Retired load instructions with L1 cache hits as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l1_miss
       [Retired load instructions missed L1 cache as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l2_hit
       [Retired load instructions with L2 cache hits as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l2_miss
       [Retired load instructions missed L2 cache as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l3_hit
       [Retired load instructions with L3 cache hits as data sources Supports
        address when precise (Precise event)]
  mem_load_retired.l3_miss
       [Retired load instructions missed L3 cache as data sources Supports
        address when precise (Precise event)]
  ocr.demand_data_rd.l3_hit.snoop_hit_with_fwd
       [OCR.DEMAND_DATA_RD.L3_HIT.SNOOP_HIT_WITH_FWD]
  ocr.demand_data_rd.l3_hit.snoop_hitm
       [Counts demand data reads that hit a cacheline in the L3 where a snoop
        hit in another cores caches, data forwarding is required as the data
        is modified]
  ocr.demand_rfo.l3_hit.snoop_hitm
       [Counts demand reads for ownership (RFO) requests and software
        prefetches for exclusive ownership (PREFETCHW) that hit a cacheline in
        the L3 where a snoop hit in another cores caches, data forwarding is
        required as the data is modified]
  offcore_requests.all_data_rd
       [Demand and prefetch data reads]
  offcore_requests.all_requests
       [Any memory transaction that reached the SQ]
  offcore_requests.demand_data_rd
       [Demand Data Read requests sent to uncore]
  offcore_requests.demand_rfo
       [Demand RFO requests including regular RFOs, locks, ItoM]
  offcore_requests_outstanding.all_data_rd
       [Offcore outstanding cacheable Core Data Read transactions in
        SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_data_rd
       [Cycles when offcore outstanding cacheable Core Data Read transactions
        are present in SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_demand_data_rd
       [Cycles when offcore outstanding Demand Data Read transactions are
        present in SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.cycles_with_demand_rfo
       [Cycles with offcore outstanding demand rfo reads transactions in
        SuperQueue (SQ), queue to uncore]
  offcore_requests_outstanding.demand_data_rd
       [Demand Data Read transactions pending for off-core. Highly correlated]
  offcore_requests_outstanding.demand_data_rd_ge_6
       [Cycles with at least 6 offcore outstanding Demand Data Read
        transactions in uncore queue]
  offcore_requests_outstanding.demand_rfo
       [Store Read transactions pending for off-core. Highly correlated]
  sq_misc.bus_lock
       [Counts bus locks, accounts for cache line split locks and UC locks]
  sq_misc.sq_full
       [Cycles the superQ cannot take any more entries]
  sw_prefetch_access.nta
       [Number of PREFETCHNTA instructions executed]
  sw_prefetch_access.prefetchw
       [Number of PREFETCHW instructions executed]
  sw_prefetch_access.t0
       [Number of PREFETCHT0 instructions executed]
  sw_prefetch_access.t1_t2
       [Number of PREFETCHT1 or PREFETCHT2 instructions executed]

floating point:
  assists.fp
       [Counts all microcode FP assists]
  fp_arith_inst_retired.128b_packed_double
       [Counts number of SSE/AVX computational 128-bit packed double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 2 computation operations,
        one for each element. Applies to SSE* and AVX* packed double precision
        floating-point instructions: ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX
        SQRT DPP FM(N)ADD/SUB. DPP and FM(N)ADD/SUB instructions count twice
        as they perform 2 calculations per element]
  fp_arith_inst_retired.128b_packed_single
       [Number of SSE/AVX computational 128-bit packed single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 4 computation operations,
        one for each element. Applies to SSE* and AVX* packed single precision
        floating-point instructions: ADD SUB MUL DIV MIN MAX RCP14 RSQRT14
        SQRT DPP FM(N)ADD/SUB. DPP and FM(N)ADD/SUB instructions count twice
        as they perform 2 calculations per element]
  fp_arith_inst_retired.256b_packed_double
       [Counts number of SSE/AVX computational 256-bit packed double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 4 computation operations,
        one for each element. Applies to SSE* and AVX* packed double precision
        floating-point instructions: ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX
        SQRT FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they
        perform 2 calculations per element]
  fp_arith_inst_retired.256b_packed_single
       [Counts number of SSE/AVX computational 256-bit packed single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 8 computation operations,
        one for each element. Applies to SSE* and AVX* packed single precision
        floating-point instructions: ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX
        SQRT RSQRT RCP DPP FM(N)ADD/SUB. DPP and FM(N)ADD/SUB instructions
        count twice as they perform 2 calculations per element]
  fp_arith_inst_retired.4_flops
       [Number of SSE/AVX computational 128-bit packed single and 256-bit
        packed double precision FP instructions retired; some instructions
        will count twice as noted below. Each count represents 2 or/and 4
        computation operations, 1 for each element. Applies to SSE* and AVX*
        packed single precision and packed double precision FP instructions:
        ADD SUB HADD HSUB SUBADD MUL DIV MIN MAX RCP14 RSQRT14 SQRT DPP
        FM(N)ADD/SUB. DPP and FM(N)ADD/SUB count twice as they perform 2
        calculations per element]
  fp_arith_inst_retired.512b_packed_double
       [Counts number of SSE/AVX computational 512-bit packed double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 8 computation operations,
        one for each element. Applies to SSE* and AVX* packed double precision
        floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT14
        RCP14 FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they
        perform 2 calculations per element]
  fp_arith_inst_retired.512b_packed_single
       [Counts number of SSE/AVX computational 512-bit packed single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 16 computation operations,
        one for each element. Applies to SSE* and AVX* packed single precision
        floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT14
        RCP14 FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they
        perform 2 calculations per element]
  fp_arith_inst_retired.8_flops
       [Number of SSE/AVX computational 256-bit packed single precision and
        512-bit packed double precision FP instructions retired; some
        instructions will count twice as noted below. Each count represents 8
        computation operations, 1 for each element. Applies to SSE* and AVX*
        packed single precision and double precision FP instructions: ADD SUB
        HADD HSUB SUBADD MUL DIV MIN MAX SQRT RSQRT RSQRT14 RCP RCP14 DPP
        FM(N)ADD/SUB. DPP and FM(N)ADD/SUB count twice as they perform 2
        calculations per element]
  fp_arith_inst_retired.scalar
       [Number of SSE/AVX computational scalar floating-point instructions
        retired; some instructions will count twice as noted below. Applies to
        SSE* and AVX* scalar, double and single precision floating-point: ADD
        SUB MUL DIV MIN MAX RCP14 RSQRT14 RANGE SQRT DPP FM(N)ADD/SUB. DPP and
        FM(N)ADD/SUB instructions count twice as they perform multiple
        calculations per element]
  fp_arith_inst_retired.scalar_double
       [Counts number of SSE/AVX computational scalar double precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 1 computational operation.
        Applies to SSE* and AVX* scalar double precision floating-point
        instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB. FM(N)ADD/SUB
        instructions count twice as they perform 2 calculations per element]
  fp_arith_inst_retired.scalar_single
       [Counts number of SSE/AVX computational scalar single precision
        floating-point instructions retired; some instructions will count
        twice as noted below. Each count represents 1 computational operation.
        Applies to SSE* and AVX* scalar single precision floating-point
        instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT RCP FM(N)ADD/SUB.
        FM(N)ADD/SUB instructions count twice as they perform 2 calculations
        per element]
  fp_arith_inst_retired.vector
       [Number of any Vector retired FP arithmetic instructions]

frontend:
  baclears.any
       [Counts the total number when the front end is resteered, mainly when
        the BPU cannot provide a correct prediction and this is corrected by
        other branch handling mechanisms at the front end]
  decode.lcp
       [Stalls caused by changing prefix length of the instruction. [This
        event is alias to ILD_STALL.LCP]]
  dsb2mite_switches.count
       [Decode Stream Buffer (DSB)-to-MITE transitions count]
  dsb2mite_switches.penalty_cycles
       [DSB-to-MITE switch true penalty cycles]
  frontend_retired.any_dsb_miss
       [Retired Instructions who experienced DSB miss (Precise event)]
  frontend_retired.dsb_miss
       [Retired Instructions who experienced a critical DSB miss (Precise
        event)]
  frontend_retired.itlb_miss
       [Retired Instructions who experienced iTLB true miss (Precise event)]
  frontend_retired.l1i_miss
       [Retired Instructions who experienced Instruction L1 Cache true miss
        (Precise event)]
  frontend_retired.l2_miss
       [Retired Instructions who experienced Instruction L2 Cache true miss
        (Precise event)]
  frontend_retired.latency_ge_1
       [Retired instructions after front-end starvation of at least 1 cycle
        (Precise event)]
  frontend_retired.latency_ge_128
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 128 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_16
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 16 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_2
       [Retired instructions after front-end starvation of at least 2 cycles
        (Precise event)]
  frontend_retired.latency_ge_256
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 256 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_2_bubbles_ge_1
       [Retired instructions that are fetched after an interval where the
        front-end had at least 1 bubble-slot for a period of 2 cycles which
        was not interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_32
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 32 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_4
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 4 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_512
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 512 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_64
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 64 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.latency_ge_8
       [Retired instructions that are fetched after an interval where the
        front-end delivered no uops for a period of 8 cycles which was not
        interrupted by a back-end stall (Precise event)]
  frontend_retired.stlb_miss
       [Retired Instructions who experienced STLB (2nd level TLB) true miss
        (Precise event)]
  icache_16b.ifdata_stall
       [Cycles where a code fetch is stalled due to L1 instruction cache miss.
        [This event is alias to ICACHE_DATA.STALLS]]
  icache_64b.iftag_hit
       [Instruction fetch tag lookups that hit in the instruction cache (L1I).
        Counts at 64-byte cache-line granularity]
  icache_64b.iftag_miss
       [Instruction fetch tag lookups that miss in the instruction cache
        (L1I). Counts at 64-byte cache-line granularity]
  icache_64b.iftag_stall
       [Cycles where a code fetch is stalled due to L1 instruction cache tag
        miss. [This event is alias to ICACHE_TAG.STALLS]]
  icache_data.stalls
       [Cycles where a code fetch is stalled due to L1 instruction cache miss.
        [This event is alias to ICACHE_16B.IFDATA_STALL]]
  icache_tag.stalls
       [Cycles where a code fetch is stalled due to L1 instruction cache tag
        miss. [This event is alias to ICACHE_64B.IFTAG_STALL]]
  idq.dsb_cycles_any
       [Cycles Decode Stream Buffer (DSB) is delivering any Uop]
  idq.dsb_cycles_ok
       [Cycles DSB is delivering optimal number of Uops]
  idq.dsb_uops
       [Uops delivered to Instruction Decode Queue (IDQ) from the Decode
        Stream Buffer (DSB) path]
  idq.mite_cycles_any
       [Cycles MITE is delivering any Uop]
  idq.mite_cycles_ok
       [Cycles MITE is delivering optimal number of Uops]
  idq.mite_uops
       [Uops delivered to Instruction Decode Queue (IDQ) from MITE path]
  idq.ms_cycles_any
       [Cycles when uops are being delivered to IDQ while MS is busy]
  idq.ms_switches
       [Number of switches from DSB or MITE to the MS]
  idq.ms_uops
       [Uops delivered to IDQ while MS is busy]
  idq_uops_not_delivered.core
       [Uops not delivered by IDQ when backend of the machine is not stalled]
  idq_uops_not_delivered.cycles_0_uops_deliv.core
       [Cycles when no uops are not delivered by the IDQ when backend of the
        machine is not stalled]
  idq_uops_not_delivered.cycles_fe_was_ok
       [Cycles when optimal number of uops was delivered to the back-end when
        the back-end is not stalled]

memory:
  cycle_activity.stalls_l3_miss
       [Execution stalls while L3 cache miss demand load is outstanding]
  machine_clears.memory_ordering
       [Number of machine clears due to memory ordering conflicts]
  mem_trans_retired.load_latency_gt_128
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 128 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_16
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 16 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_256
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 256 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_32
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 32 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_4
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 4 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_512
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 512 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_64
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 64 cycles Supports address when precise
        (Must be precise)]
  mem_trans_retired.load_latency_gt_8
       [Counts randomly selected loads when the latency from first dispatch to
        completion is greater than 8 cycles Supports address when precise
        (Must be precise)]
  offcore_requests.l3_miss_demand_data_rd
       [Demand Data Read requests who miss L3 cache]
  rtm_retired.aborted
       [Number of times an RTM execution aborted]
  rtm_retired.aborted_events
       [Number of times an RTM execution aborted due to none of the previous 4
        categories (e.g. interrupt)]
  rtm_retired.aborted_mem
       [Number of times an RTM execution aborted due to various memory events
        (e.g. read/write capacity and conflicts)]
  rtm_retired.aborted_memtype
       [Number of times an RTM execution aborted due to incompatible memory
        type]
  rtm_retired.aborted_unfriendly
       [Number of times an RTM execution aborted due to HLE-unfriendly
        instructions]
  rtm_retired.commit
       [Number of times an RTM execution successfully committed]
  rtm_retired.start
       [Number of times an RTM execution started]
  tx_exec.misc2
       [Counts the number of times a class of instructions that may cause a
        transactional abort was executed inside a transactional region]
  tx_exec.misc3
       [Number of times an instruction execution caused the transactional nest
        count supported to be exceeded]
  tx_mem.abort_capacity_read
       [Speculatively counts the number of TSX aborts due to a data capacity
        limitation for transactional reads]
  tx_mem.abort_capacity_write
       [Speculatively counts the number of TSX aborts due to a data capacity
        limitation for transactional writes]
  tx_mem.abort_conflict
       [Number of times a transactional abort was signaled due to a data
        conflict on a transactionally accessed address]

other:
  core_power.lvl0_turbo_license
       [Core cycles where the core was running in a manner where Turbo may be
        clipped to the Non-AVX turbo schedule]
  core_power.lvl1_turbo_license
       [Core cycles where the core was running in a manner where Turbo may be
        clipped to the AVX2 turbo schedule]
  core_power.lvl2_turbo_license
       [Core cycles where the core was running in a manner where Turbo may be
        clipped to the AVX512 turbo schedule]
  ocr.streaming_wr.any_response
       [Counts streaming stores that have any type of response]

pipeline:
  arith.divider_active
       [Cycles when divide unit is busy executing divide or square root
        operations]
  assists.any
       [Number of occurrences where a microcode assist is invoked by hardware]
  br_inst_retired.all_branches
       [All branch instructions retired (Precise event)]
  br_inst_retired.cond
       [Conditional branch instructions retired (Precise event)]
  br_inst_retired.cond_ntaken
       [Not taken branch instructions retired (Precise event)]
  br_inst_retired.cond_taken
       [Taken conditional branch instructions retired (Precise event)]
  br_inst_retired.far_branch
       [Far branch instructions retired (Precise event)]
  br_inst_retired.indirect
       [Indirect near branch instructions retired (excluding returns) (Precise
        event)]
  br_inst_retired.near_call
       [Direct and indirect near call instructions retired (Precise event)]
  br_inst_retired.near_return
       [Return instructions retired (Precise event)]
  br_inst_retired.near_taken
       [Taken branch instructions retired (Precise event)]
  br_misp_retired.all_branches
       [All mispredicted branch instructions retired (Precise event)]
  br_misp_retired.cond
       [Mispredicted conditional branch instructions retired (Precise event)]
  br_misp_retired.cond_ntaken
       [Mispredicted non-taken conditional branch instructions retired
        (Precise event)]
  br_misp_retired.cond_taken
       [number of branch instructions retired that were mispredicted and taken
        (Precise event)]
  br_misp_retired.indirect
       [All miss-predicted indirect branch instructions retired (excluding
        RETs. TSX aborts is considered indirect branch) (Precise event)]
  br_misp_retired.indirect_call
       [Mispredicted indirect CALL instructions retired (Precise event)]
  br_misp_retired.near_taken
       [Number of near branch instructions retired that were mispredicted and
        taken (Precise event)]
  br_misp_retired.ret
       [This event counts the number of mispredicted ret instructions retired.
        Non PEBS (Precise event)]
  cpu_clk_unhalted.distributed
       [Cycle counts are evenly distributed between active threads in the Core]
  cpu_clk_unhalted.one_thread_active
       [Core crystal clock cycles when this thread is unhalted and the other
        thread is halted]
  cpu_clk_unhalted.ref_distributed
       [Core crystal clock cycles. Cycle counts are evenly distributed between
        active threads in the Core]
  cpu_clk_unhalted.ref_tsc
       [Reference cycles when the core is not in halt state]
  cpu_clk_unhalted.ref_xclk
       [Core crystal clock cycles when the thread is unhalted]
  cpu_clk_unhalted.thread
       [Core cycles when the thread is not in halt state]
  cpu_clk_unhalted.thread_p
       [Thread cycles when thread is not in halt state]
  cycle_activity.cycles_l1d_miss
       [Cycles while L1 cache miss demand load is outstanding]
  cycle_activity.cycles_l2_miss
       [Cycles while L2 cache miss demand load is outstanding]
  cycle_activity.cycles_mem_any
       [Cycles while memory subsystem has an outstanding load]
  cycle_activity.stalls_l1d_miss
       [Execution stalls while L1 cache miss demand load is outstanding]
  cycle_activity.stalls_l2_miss
       [Execution stalls while L2 cache miss demand load is outstanding]
  cycle_activity.stalls_mem_any
       [Execution stalls while memory subsystem has an outstanding load]
  cycle_activity.stalls_total
       [Total execution stalls]
  exe_activity.1_ports_util
       [Cycles total of 1 uop is executed on all ports and Reservation Station
        was not empty]
  exe_activity.2_ports_util
       [Cycles total of 2 uops are executed on all ports and Reservation
        Station was not empty]
  exe_activity.3_ports_util
       [Cycles total of 3 uops are executed on all ports and Reservation
        Station was not empty]
  exe_activity.4_ports_util
       [Cycles total of 4 uops are executed on all ports and Reservation
        Station was not empty]
  exe_activity.bound_on_loads
       [Cycles when the memory subsystem has an outstanding load. Increments
        by 4 for every such cycle]
  exe_activity.bound_on_stores
       [Cycles where the Store Buffer was full and no loads caused an
        execution stall]
  exe_activity.exe_bound_0_ports
       [Cycles no uop executed while RS was not empty, the SB was not full and
        there was no outstanding load]
  ild_stall.lcp
       [Stalls caused by changing prefix length of the instruction. [This
        event is alias to DECODE.LCP]]
  inst_decoded.decoders
       [Instruction decoders utilized in a cycle]
  inst_retired.any
       [Number of instructions retired. Fixed Counter - architectural event
        (Precise event)]
  inst_retired.any_p
       [Number of instructions retired. General Counter - architectural event
        (Precise event)]
  inst_retired.nop
       [Retired NOP instructions (Precise event)]
  inst_retired.prec_dist
       [Precise instruction retired event with a reduced effect of PEBS shadow
        in IP distribution (Precise event)]
  int_misc.all_recovery_cycles
       [Cycles the Backend cluster is recovering after a miss-speculation or a
        Store Buffer or Load Buffer drain stall]
  int_misc.clear_resteer_cycles
       [Counts cycles after recovery from a branch misprediction or machine
        clear till the first uop is issued from the resteered path]
  int_misc.clears_count
       [Clears speculative count]
  int_misc.recovery_cycles
       [Core cycles the allocator was stalled due to recovery from earlier
        clear event for this thread]
  int_misc.uop_dropping
       [TMA slots where uops got dropped]
  ld_blocks.no_sr
       [The number of times that split load operations are temporarily blocked
        because all resources for handling the split accesses are in use]
  ld_blocks.store_forward
       [Loads blocked due to overlapping with a preceding store that cannot be
        forwarded]
  ld_blocks_partial.address_alias
       [False dependencies in MOB due to partial compare on address]
  load_hit_prefetch.swpf
       [Counts the number of demand load dispatches that hit L1D fill buffer
        (FB) allocated for software prefetch]
  lsd.cycles_active
       [Cycles Uops delivered by the LSD, but didn't come from the decoder]
  lsd.cycles_ok
       [Cycles optimal number of Uops delivered by the LSD, but did not come
        from the decoder]
  lsd.uops
       [Number of Uops delivered by the LSD]
  machine_clears.count
       [Number of machine clears (nukes) of any type]
  machine_clears.smc
       [Self-modifying code (SMC) detected]
  misc_retired.lbr_inserts
       [Increments whenever there is an update to the LBR array]
  misc_retired.pause_inst
       [Number of retired PAUSE instructions. This event is not supported on
        first SKL and KBL products]
  resource_stalls.sb
       [Cycles stalled due to no store buffers available. (not including
        draining form sync)]
  resource_stalls.scoreboard
       [Counts cycles where the pipeline is stalled due to serializing
        operations]
  rs_events.empty_cycles
       [Cycles when Reservation Station (RS) is empty for the thread]
  rs_events.empty_end
       [Counts end of periods where the Reservation Station (RS) was empty]
  topdown.backend_bound_slots
       [TMA slots where no uops were being issued due to lack of back-end
        resources]
  topdown.br_mispredict_slots
       [TMA slots wasted due to incorrect speculation by branch mispredictions]
  topdown.slots
       [TMA slots available for an unhalted logical processor. Fixed counter -
        architectural event]
  topdown.slots_p
       [TMA slots available for an unhalted logical processor. General counter
        - architectural event]
  uops_decoded.dec0
       [Number of uops decoded out of instructions exclusively fetched by
        decoder 0]
  uops_dispatched.port_0
       [Number of uops executed on port 0]
  uops_dispatched.port_1
       [Number of uops executed on port 1]
  uops_dispatched.port_2_3
       [Number of uops executed on port 2 and 3]
  uops_dispatched.port_4_9
       [Number of uops executed on port 4 and 9]
  uops_dispatched.port_5
       [Number of uops executed on port 5]
  uops_dispatched.port_6
       [Number of uops executed on port 6]
  uops_dispatched.port_7_8
       [Number of uops executed on port 7 and 8]
  uops_executed.core
       [Number of uops executed on the core]
  uops_executed.core_cycles_ge_1
       [Cycles at least 1 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_ge_2
       [Cycles at least 2 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_ge_3
       [Cycles at least 3 micro-op is executed from any thread on physical
        core]
  uops_executed.core_cycles_ge_4
       [Cycles at least 4 micro-op is executed from any thread on physical
        core]
  uops_executed.cycles_ge_1
       [Cycles where at least 1 uop was executed per-thread]
  uops_executed.cycles_ge_2
       [Cycles where at least 2 uops were executed per-thread]
  uops_executed.cycles_ge_3
       [Cycles where at least 3 uops were executed per-thread]
  uops_executed.cycles_ge_4
       [Cycles where at least 4 uops were executed per-thread]
  uops_executed.stall_cycles
       [Counts number of cycles no uops were dispatched to be executed on this
        thread]
  uops_executed.thread
       [Counts the number of uops to be executed per-thread each cycle]
  uops_executed.x87
       [Counts the number of x87 uops dispatched]
  uops_issued.any
       [Uops that RAT issues to RS]
  uops_issued.stall_cycles
       [Cycles when RAT does not issue Uops to RS for the thread]
  uops_issued.vector_width_mismatch
       [Uops inserted at issue-stage in order to preserve upper bits of vector
        registers]
  uops_retired.slots
       [Retirement slots used]
  uops_retired.stall_cycles
       [Cycles without actually retired uops]
  uops_retired.total_cycles
       [Cycles with less than 10 actually retired uops]

uncore interconnect:
  unc_arb_coh_trk_requests.all
       [UNC_ARB_COH_TRK_REQUESTS.ALL. Unit: uncore_arb]
  unc_arb_dat_occupancy.all
       [Each cycle counts number of any coherent request at memory controller
        that were issued by any core. Unit: uncore_arb]
  unc_arb_dat_occupancy.rd
       [Each cycle counts number of coherent reads pending on data return from
        memory controller that were issued by any core. Unit: uncore_arb]
  unc_arb_dat_requests.rd
       [This event is deprecated. Refer to new event
        UNC_ARB_REQ_TRK_REQUEST.DRD. Unit: uncore_arb]
  unc_arb_ifa_occupancy.all
       [This event is deprecated. Refer to new event
        UNC_ARB_DAT_OCCUPANCY.ALL. Unit: uncore_arb]
  unc_arb_req_trk_occupancy.drd
       [Each cycle count number of 'valid' coherent Data Read entries . Such
        entry is defined as valid when it is allocated till deallocation.
        Doesn't include prefetches [This event is alias to
        UNC_ARB_TRK_OCCUPANCY.RD]. Unit: uncore_arb]
  unc_arb_req_trk_request.drd
       [Number of all coherent Data Read entries. Doesn't include prefetches
        [This event is alias to UNC_ARB_TRK_REQUESTS.RD]. Unit: uncore_arb]
  unc_arb_trk_occupancy.all
       [Each cycle count number of all outgoing valid entries in ReqTrk. Such
        entry is defined as valid from it's allocation in ReqTrk till
        deallocation. Accounts for Coherent and non-coherent traffic. Unit:
        uncore_arb]
  unc_arb_trk_occupancy.rd
       [Each cycle count number of 'valid' coherent Data Read entries . Such
        entry is defined as valid when it is allocated till deallocation.
        Doesn't include prefetches [This event is alias to
        UNC_ARB_REQ_TRK_OCCUPANCY.DRD]. Unit: uncore_arb]
  unc_arb_trk_requests.all
       [UNC_ARB_TRK_REQUESTS.ALL. Unit: uncore_arb]
  unc_arb_trk_requests.rd
       [Number of all coherent Data Read entries. Doesn't include prefetches
        [This event is alias to UNC_ARB_REQ_TRK_REQUEST.DRD]. Unit: uncore_arb]

uncore memory:
  unc_mc0_rdcas_count_freerun
       [Counts every read (RdCAS) issued by the Memory Controller to DRAM (sum
        of all channels). All requests result in 64 byte data transfers from
        DRAM. Unit: uncore_imc_free_running_0]
  unc_mc0_total_reqcount_freerun
       [Counts every 64B read and write request entering the Memory Controller
        to DRAM (sum of all channels). Each write request counts as a new
        request incrementing this counter. However, same cache line write
        requests (both full and partial) are combined to a single 64 byte data
        transfer to DRAM. Unit: uncore_imc_free_running_0]
  unc_mc0_wrcas_count_freerun
       [Counts every write (WrCAS) issued by the Memory Controller to DRAM
        (sum of all channels). All requests result in 64 byte data transfers
        from DRAM. Unit: uncore_imc_free_running_0]

uncore other:
  unc_clock.socket
       [UNC_CLOCK.SOCKET. Unit: uncore_clock]

virtual memory:
  dtlb_load_misses.stlb_hit
       [Loads that miss the DTLB and hit the STLB]
  dtlb_load_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for a demand
        load]
  dtlb_load_misses.walk_completed
       [Load miss in all TLB levels causes a page walk that completes. (All
        page sizes)]
  dtlb_load_misses.walk_completed_2m_4m
       [Page walks completed due to a demand data load to a 2M/4M page]
  dtlb_load_misses.walk_completed_4k
       [Page walks completed due to a demand data load to a 4K page]
  dtlb_load_misses.walk_pending
       [Number of page walks outstanding for a demand load in the PMH each
        cycle]
  dtlb_store_misses.stlb_hit
       [Stores that miss the DTLB and hit the STLB]
  dtlb_store_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for a store]
  dtlb_store_misses.walk_completed
       [Store misses in all TLB levels causes a page walk that completes. (All
        page sizes)]
  dtlb_store_misses.walk_completed_2m_4m
       [Page walks completed due to a demand data store to a 2M/4M page]
  dtlb_store_misses.walk_completed_4k
       [Page walks completed due to a demand data store to a 4K page]
  dtlb_store_misses.walk_pending
       [Number of page walks outstanding for a store in the PMH each cycle]
  itlb_misses.stlb_hit
       [Instruction fetch requests that miss the ITLB and hit the STLB]
  itlb_misses.walk_active
       [Cycles when at least one PMH is busy with a page walk for code
        (instruction fetch) request]
  itlb_misses.walk_completed
       [Code miss in all TLB levels causes a page walk that completes. (All
        page sizes)]
  itlb_misses.walk_completed_2m_4m
       [Code miss in all TLB levels causes a page walk that completes. (2M/4M)]
  itlb_misses.walk_completed_4k
       [Code miss in all TLB levels causes a page walk that completes. (4K)]
  itlb_misses.walk_pending
       [Number of page walks outstanding for an outstanding code request in
        the PMH each cycle]
  tlb_flush.dtlb_thread
       [DTLB flush attempts of the thread-specific entries]
  tlb_flush.stlb_any
       [STLB flush attempts]
  rNNN                                               [Raw hardware event descriptor]
  cpu/t1=v1[,t2=v2,t3 ...]/modifier                  [Raw hardware event descriptor]
       [(see 'man perf-list' on how to encode it)]
  mem:<addr>[/len][:access]                          [Hardware breakpoint]

Metric Groups:

Backend: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

Bad: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bad_spec_ipmisp_cond_ntaken
       [Instructions per retired mispredicts for conditional non-taken
        branches (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_cond_taken
       [Instructions per retired mispredicts for conditional taken branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_indirect
       [Instructions per retired mispredicts for indirect CALL or JMP branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_ret
       [Instructions per retired mispredicts for return branches (lower number
        means higher occurrence rate)]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_info_branches_callret
       [Fraction of branches that are CALL or RET]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_branches_jump
       [Fraction of branches that are unconditional (direct or indirect) jumps]
  tma_info_branches_other_branches
       [Fraction of branches of other types (not individually covered by other
        metrics in Info.Branches group)]

BadSpec:
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

BigFoot: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

BrMispredicts: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bad_spec_ipmisp_cond_ntaken
       [Instructions per retired mispredicts for conditional non-taken
        branches (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_cond_taken
       [Instructions per retired mispredicts for conditional taken branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_indirect
       [Instructions per retired mispredicts for indirect CALL or JMP branches
        (lower number means higher occurrence rate)]
  tma_info_bad_spec_ipmisp_ret
       [Instructions per retired mispredicts for return branches (lower number
        means higher occurrence rate)]
  tma_info_bad_spec_ipmispredict
       [Number of Instructions per non-speculative Branch Misprediction
        (JEClear) (lower number means higher occurrence rate)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

Branches: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_branches_callret
       [Fraction of branches that are CALL or RET]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_branches_jump
       [Fraction of branches that are unconditional (direct or indirect) jumps]
  tma_info_branches_other_branches
       [Fraction of branches of other types (not individually covered by other
        metrics in Info.Branches group)]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_info_system_ipfarbranch
       [Instructions per Far Branch ( Far Branches apply upon transition from
        application to operating system, handling interrupts, exceptions)
        [lower number means higher occurrence rate]]
  tma_info_thread_uptb
       [Instruction per taken branch]

CacheMisses: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_fb_hpki
       [Fill Buffer (FB) hits per kilo instructions for retired demand loads
        (L1D misses that merge into ongoing miss-handling entries)]
  tma_info_memory_l1mpki
       [L1 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l1mpki_load
       [L1 cache true misses per kilo instruction for all demand loads
        (including speculative)]
  tma_info_memory_l2hpki_all
       [L2 cache hits per kilo instruction for all request types (including
        speculative)]
  tma_info_memory_l2hpki_load
       [L2 cache hits per kilo instruction for all demand loads (including
        speculative)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_load
       [L2 cache ([RKL+] true) misses per kilo instruction for all demand
        loads (including speculative)]
  tma_info_memory_l3mpki
       [L3 cache true misses per kilo instruction for retired demand loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]

CodeGen: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]

Compute: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

Cor: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l0_core_bound_likely
       [Probability of Core Bound bottleneck hidden by SMT-profiling artifacts]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]
  tma_info_thread_execute_per_issue
       [The ratio of Executed- by Issued-Uops]

DSB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]

DSBmiss: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_switch_cost
       [Average number of cycles of a switch from the DSB fetch-unit to MITE
        fetch unit - see DSB_Switches tree node for details]
  tma_info_frontend_ipdsb_miss_ret
       [Instructions per non-speculative DSB miss (lower number means higher
        occurrence rate)]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]

DataSharing: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]

Default:
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

Fed: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_frontend_fetch_upc
       [Average number of Uops issued by front-end when it issued something]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_info_frontend_ipdsb_miss_ret
       [Instructions per non-speculative DSB miss (lower number means higher
        occurrence rate)]
  tma_info_frontend_ipunknown_branch
       [Instructions per speculative Unknown Branch Misprediction (BAClear)
        (lower number means higher occurrence rate)]
  tma_info_frontend_lsd_coverage
       [Fraction of Uops delivered by the LSD (Loop Stream Detector; aka Loop
        Cache)]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_info_memory_tlb_code_stlb_mpki
       [STLB (2nd level TLB) code speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_thread_uptb
       [Instruction per taken branch]

FetchBW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_frontend_fetch_upc
       [Average number of Uops issued by front-end when it issued something]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_info_thread_uptb
       [Instruction per taken branch]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]

FetchLat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

Flops: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_info_core_flopc
       [Floating Point Operations Per Cycle]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_inst_mix_iparith
       [Instructions per FP Arithmetic instruction (lower number means higher
        occurrence rate)]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx512
       [Instructions per FP Arithmetic AVX 512-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipflop
       [Instructions per Floating Point (FP) Operation (lower number means
        higher occurrence rate)]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]

FpScalar: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]

FpVector: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx512
       [Instructions per FP Arithmetic AVX 512-bit instruction (lower number
        means higher occurrence rate)]

Frontend: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_instruction_fetch_bw
       [Total pipeline cost of instruction fetch bandwidth related bottlenecks]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]

HPC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_info_core_fp_arith_utilization
       [Actual per-core usage of the Floating Point non-X87 execution units
        (regardless of precision or vector-width)]
  tma_info_system_cpu_utilization
       [Average CPU Utilization]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_gflops
       [Giga Floating Point Operations Per Second]

IcMiss: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_frontend_icache_miss_latency
       [Average Latency for L1 instruction cache misses]
  tma_info_frontend_l2mpki_code
       [L2 cache true code cacheline misses per kilo instruction]
  tma_info_frontend_l2mpki_code_all
       [L2 cache speculative code cacheline misses per kilo instruction]

InsType: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_iparith
       [Instructions per FP Arithmetic instruction (lower number means higher
        occurrence rate)]
  tma_info_inst_mix_iparith_avx128
       [Instructions per FP Arithmetic AVX/SSE 128-bit instruction (lower
        number means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx256
       [Instructions per FP Arithmetic AVX* 256-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_avx512
       [Instructions per FP Arithmetic AVX 512-bit instruction (lower number
        means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_dp
       [Instructions per FP Arithmetic Scalar Double-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_iparith_scalar_sp
       [Instructions per FP Arithmetic Scalar Single-Precision instruction
        (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipbranch
       [Instructions per Branch (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipflop
       [Instructions per Floating Point (FP) Operation (lower number means
        higher occurrence rate)]
  tma_info_inst_mix_ipload
       [Instructions per Load (lower number means higher occurrence rate)]
  tma_info_inst_mix_ipstore
       [Instructions per Store (lower number means higher occurrence rate)]

LSD: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_frontend_lsd_coverage
       [Fraction of Uops delivered by the LSD (Loop Stream Detector; aka Loop
        Cache)]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]

MachineClears: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

Mem: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_info_memory_core_l1d_cache_fill_bw
       [Average per-core data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_core_l2_cache_fill_bw
       [Average per-core data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_core_l3_cache_access_bw
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_core_l3_cache_fill_bw
       [Average per-core data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_fb_hpki
       [Fill Buffer (FB) hits per kilo instructions for retired demand loads
        (L1D misses that merge into ongoing miss-handling entries)]
  tma_info_memory_l1mpki
       [L1 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l1mpki_load
       [L1 cache true misses per kilo instruction for all demand loads
        (including speculative)]
  tma_info_memory_l2hpki_all
       [L2 cache hits per kilo instruction for all request types (including
        speculative)]
  tma_info_memory_l2hpki_load
       [L2 cache hits per kilo instruction for all demand loads (including
        speculative)]
  tma_info_memory_l2mpki
       [L2 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_l2mpki_load
       [L2 cache ([RKL+] true) misses per kilo instruction for all demand
        loads (including speculative)]
  tma_info_memory_l3mpki
       [L3 cache true misses per kilo instruction for retired demand loads]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_info_memory_thread_l1d_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_thread_l2_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_access_bw_1t
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_tlb_load_stlb_mpki
       [STLB (2nd level TLB) data load speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_page_walks_utilization
       [Utilization of the core's Page Walker(s) serving STLB misses triggered
        by instruction/Load/Store accesses]
  tma_info_memory_tlb_store_stlb_mpki
       [STLB (2nd level TLB) data store speculative misses per kilo
        instruction (misses of any page-size that complete the page walk)]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_info_system_mem_request_latency
       [Average latency of all requests to external memory (in Uncore cycles)]
  tma_info_thread_cpi
       [Cycles Per Instruction (per Logical Processor)]

MemoryBW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_memory_core_l1d_cache_fill_bw
       [Average per-core data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_core_l2_cache_fill_bw
       [Average per-core data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_core_l3_cache_access_bw
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_core_l3_cache_fill_bw
       [Average per-core data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_info_memory_thread_l1d_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L1 data cache [GB / sec]]
  tma_info_memory_thread_l2_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L2 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_access_bw_1t
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_thread_l3_cache_fill_bw_1t
       [Average per-thread data fill bandwidth to the L3 cache [GB / sec]]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

MemoryBound: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_memory_mlp
       [Memory-Level-Parallelism (average number of L1 miss demand load when
        there is at least one such miss]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

MemoryLat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_info_memory_load_miss_real_latency
       [Actual Average Latency for L1 data-cache miss demand load operations
        (in core cycles)]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

MemoryTLB: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_memory_tlb_code_stlb_mpki
       [STLB (2nd level TLB) code speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_load_stlb_mpki
       [STLB (2nd level TLB) data load speculative misses per kilo instruction
        (misses of any page-size that complete the page walk)]
  tma_info_memory_tlb_page_walks_utilization
       [Utilization of the core's Page Walker(s) serving STLB misses triggered
        by instruction/Load/Store accesses]
  tma_info_memory_tlb_store_stlb_mpki
       [STLB (2nd level TLB) data store speculative misses per kilo
        instruction (misses of any page-size that complete the page walk)]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

Memory_BW: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_oro_data_l2_mlp
       [Average Parallel L2 cache miss data reads]
  tma_info_memory_oro_load_l2_mlp
       [Average Parallel L2 cache miss demand Loads]

Memory_Lat: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_memory_oro_load_l2_miss_latency
       [Average Latency for L2 cache miss demand Loads]
  tma_info_memory_oro_load_l3_miss_latency
       [Average Latency for L3 cache miss demand Loads]

MicroSeq: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

OS: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_system_ipfarbranch
       [Instructions per Far Branch ( Far Branches apply upon transition from
        application to operating system, handling interrupts, exceptions)
        [lower number means higher occurrence rate]]
  tma_info_system_kernel_cpi
       [Cycles Per Instruction for the Operating System (OS) Kernel mode]
  tma_info_system_kernel_utilization
       [Fraction of cycles spent in the Operating System (OS) Kernel mode]

Offcore: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_info_memory_core_l3_cache_access_bw
       [Average per-core data access bandwidth to the L3 cache [GB / sec]]
  tma_info_memory_l2mpki_all
       [L2 cache ([RKL+] true) misses per kilo instruction for all request
        types (including speculative)]
  tma_info_memory_oro_data_l2_mlp
       [Average Parallel L2 cache miss data reads]
  tma_info_memory_oro_load_l2_miss_latency
       [Average Latency for L2 cache miss demand Loads]
  tma_info_memory_oro_load_l2_mlp
       [Average Parallel L2 cache miss demand Loads]
  tma_info_memory_oro_load_l3_miss_latency
       [Average Latency for L3 cache miss demand Loads]
  tma_info_memory_thread_l3_cache_access_bw_1t
       [Average per-thread data access bandwidth to the L3 cache [GB / sec]]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

PGO: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_branches_cond_nt
       [Fraction of branches that are non-taken conditionals]
  tma_info_branches_cond_tk
       [Fraction of branches that are taken conditionals]
  tma_info_inst_mix_bptkbranch
       [Branch instructions per taken branch]
  tma_info_inst_mix_ipcall
       [Instructions per (near) call (lower number means higher occurrence
        rate)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]

Pipeline: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_info_pipeline_retire
       [Average number of Uops retired in cycles where at least one uop has
        retired]
  tma_info_thread_clks
       [Per-Logical Processor actual clocks when the Logical Processor is
        active]
  tma_info_thread_cpi
       [Cycles Per Instruction (per Logical Processor)]
  tma_info_thread_execute_per_issue
       [The ratio of Executed- by Issued-Uops]
  tma_info_thread_uoppi
       [Uops Per Instruction]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]

PortsUtil: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_core_ilp
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-core]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

Power: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  C10_Pkg_Residency
       [C10 residency percent per package]
  C2_Pkg_Residency
       [C2 residency percent per package]
  C3_Pkg_Residency
       [C3 residency percent per package]
  C6_Core_Residency
       [C6 residency percent per core]
  C6_Pkg_Residency
       [C6 residency percent per package]
  C7_Core_Residency
       [C7 residency percent per core]
  C7_Pkg_Residency
       [C7 residency percent per package]
  C8_Pkg_Residency
       [C8 residency percent per package]
  C9_Pkg_Residency
       [C9 residency percent per package]
  tma_info_system_average_frequency
       [Measured Average Frequency for unhalted processors [GHz]]
  tma_info_system_power_license0_utilization
       [Fraction of Core cycles where the core was running with power-delivery
        for baseline license level 0]
  tma_info_system_power_license1_utilization
       [Fraction of Core cycles where the core was running with power-delivery
        for license level 1]
  tma_info_system_power_license2_utilization
       [Fraction of Core cycles where the core was running with power-delivery
        for license level 2 (introduced in SKX)]
  tma_info_system_turbo_utilization
       [Average Frequency Utilization relative nominal frequency]

Prefetches: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_ipswpf
       [Instructions per Software prefetch instruction (of any type:
        NTA/T0/T1/T2/Prefetch) (lower number means higher occurrence rate)]

Ret: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_bottleneck_branching_overhead
       [Total pipeline cost of branch related instructions (used for program
        control-flow including function calls)]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_core_flopc
       [Floating Point Operations Per Cycle]
  tma_info_pipeline_retire
       [Average number of Uops retired in cycles where at least one uop has
        retired]
  tma_info_thread_ipc
       [Instructions Per Cycle (per Logical Processor)]
  tma_info_thread_uoppi
       [Uops Per Instruction]

Retire: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_info_thread_uoppi
       [Uops Per Instruction]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]

SMT: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_botlnk_l0_core_bound_likely
       [Probability of Core Bound bottleneck hidden by SMT-profiling artifacts]
  tma_info_core_core_clks
       [Core actual clocks when any Logical Processor is active on the
        Physical Core]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_pipeline_execute
       [Instruction-Level-Parallelism (average number of uops executed when
        there is execution) per-thread]
  tma_info_system_smt_2t_utilization
       [Fraction of cycles where both hardware Logical Processors were active]
  tma_info_thread_slots_utilization
       [Fraction of Physical Core issue-slots utilized by this Logical
        Processor]

Snoop: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]

SoC: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  UNCORE_FREQ
       [Uncore frequency per die [GHZ]]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_info_system_mem_parallel_reads
       [Average number of parallel data read requests to external memory]
  tma_info_system_mem_read_latency
       [Average latency of data read request to external memory (in
        nanoseconds)]
  tma_info_system_mem_request_latency
       [Average latency of all requests to external memory (in Uncore cycles)]
  tma_info_system_socket_clks
       [Socket actual clocks when any core is active on that socket]

Summary: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_system_average_frequency
       [Measured Average Frequency for unhalted processors [GHz]]
  tma_info_system_cpu_utilization
       [Average CPU Utilization]
  tma_info_thread_ipc
       [Instructions Per Cycle (per Logical Processor)]

TmaL1: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_thread_slots
       [Total issue-pipeline slots (per-Physical Core till ICL; per-Logical
        Processor ICL onward)]
  tma_info_thread_slots_utilization
       [Fraction of Physical Core issue-slots utilized by this Logical
        Processor]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

TmaL2: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

TmaL3mem: [Grouping from Top-down Microarchitecture Analysis Metrics spreadsheet]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

TopdownL1: [Metrics for top-down breakdown at level 1]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

TopdownL2: [Metrics for top-down breakdown at level 2]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

TopdownL3: [Metrics for top-down breakdown at level 3]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

TopdownL4: [Metrics for top-down breakdown at level 4]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

TopdownL5: [Metrics for top-down breakdown at level 5]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

TopdownL6: [Metrics for top-down breakdown at level 6]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]
  tma_slow_pause
       [This metric represents fraction of cycles the CPU was stalled due to
        PAUSE Instructions]

smi:
  smi_cycles
       [Percentage of cycles spent in System Management Interrupts]
  smi_num
       [Number of SMI interrupts]

tma_L1_group: [Metrics for top-down breakdown at level 1]
  tma_backend_bound
       [This category represents fraction of slots where no uops are being
        delivered due to a lack of required resources for accepting new uops
        in the Backend]
  tma_bad_speculation
       [This category represents fraction of slots wasted due to incorrect
        speculations]
  tma_frontend_bound
       [This category represents fraction of slots where the processor's
        Frontend undersupplies its Backend]
  tma_info_core_coreipc
       [Instructions Per Cycle across hyper-threads (per physical core)]
  tma_info_inst_mix_instructions
       [Total number of retired Instructions]
  tma_info_thread_slots
       [Total issue-pipeline slots (per-Physical Core till ICL; per-Logical
        Processor ICL onward)]
  tma_info_thread_slots_utilization
       [Fraction of Physical Core issue-slots utilized by this Logical
        Processor]
  tma_retiring
       [This category represents fraction of slots utilized by useful work
        i.e. issued uops that eventually get retired]

tma_L2_group: [Metrics for top-down breakdown at level 2]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

tma_L3_group: [Metrics for top-down breakdown at level 3]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

tma_L4_group: [Metrics for top-down breakdown at level 4]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

tma_L5_group: [Metrics for top-down breakdown at level 5]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

tma_L6_group: [Metrics for top-down breakdown at level 6]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]
  tma_slow_pause
       [This metric represents fraction of cycles the CPU was stalled due to
        PAUSE Instructions]

tma_alu_op_utilization_group: [Metrics contributing to tma_alu_op_utilization category]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]

tma_backend_bound_group: [Metrics contributing to tma_backend_bound category]
  tma_core_bound
       [This metric represents fraction of slots where Core non-memory issues
        were of a bottleneck]
  tma_memory_bound
       [This metric represents fraction of slots the Memory subsystem within
        the Backend was a bottleneck]

tma_bad_speculation_group: [Metrics contributing to tma_bad_speculation category]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

tma_branch_resteers_group: [Metrics contributing to tma_branch_resteers category]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]
  tma_unknown_branches
       [This metric represents fraction of cycles the CPU was stalled due to
        new branch address clears]

tma_core_bound_group: [Metrics contributing to tma_core_bound category]
  tma_divider
       [This metric represents fraction of cycles where the Divider unit was
        active]
  tma_ports_utilization
       [This metric estimates fraction of cycles the CPU performance was
        potentially limited due to Core computation issues (non
        divider-related)]

tma_dram_bound_group: [Metrics contributing to tma_dram_bound category]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]

tma_dtlb_load_group: [Metrics contributing to tma_dtlb_load category]
  tma_load_stlb_hit
       [This metric roughly estimates the fraction of cycles where the (first
        level) DTLB was missed by load accesses, that later on hit in
        second-level TLB (STLB)]
  tma_load_stlb_miss
       [This metric estimates the fraction of cycles where the Second-level
        TLB (STLB) was missed by load accesses, performing a hardware page
        walk]

tma_dtlb_store_group: [Metrics contributing to tma_dtlb_store category]
  tma_store_stlb_hit
       [This metric roughly estimates the fraction of cycles where the TLB was
        missed by store accesses, hitting in the second-level TLB (STLB)]
  tma_store_stlb_miss
       [This metric estimates the fraction of cycles where the STLB was missed
        by store accesses, performing a hardware page walk]

tma_fetch_bandwidth_group: [Metrics contributing to tma_fetch_bandwidth category]
  tma_dsb
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to DSB (decoded uop cache) fetch pipeline]
  tma_lsd
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to LSD (Loop Stream Detector) unit]
  tma_mite
       [This metric represents Core fraction of cycles in which CPU was likely
        limited due to the MITE pipeline (the legacy decode pipeline)]

tma_fetch_latency_group: [Metrics contributing to tma_fetch_latency category]
  tma_branch_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_icache_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        instruction cache misses]
  tma_itlb_misses
       [This metric represents fraction of cycles the CPU was stalled due to
        Instruction TLB (ITLB) misses]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_fp_arith_group: [Metrics contributing to tma_fp_arith category]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_x87_use
       [This metric serves as an approximation of legacy x87 usage]

tma_fp_vector_group: [Metrics contributing to tma_fp_vector category]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]

tma_frontend_bound_group: [Metrics contributing to tma_frontend_bound category]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_fetch_latency
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend latency issues]

tma_heavy_operations_group: [Metrics contributing to tma_heavy_operations category]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]

tma_issue2P: [Metrics related by the issue $issue2P]
  tma_fp_scalar
       [This metric approximates arithmetic floating-point (FP) scalar uops
        fraction the CPU has retired]
  tma_fp_vector
       [This metric approximates arithmetic floating-point (FP) vector uops
        fraction the CPU has retired aggregated across all vector widths]
  tma_fp_vector_128b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 128-bit wide vectors]
  tma_fp_vector_256b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 256-bit wide vectors]
  tma_fp_vector_512b
       [This metric approximates arithmetic FP vector uops fraction the CPU
        has retired for 512-bit wide vectors]
  tma_port_0
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 0 ([SNB+] ALU; [HSW+] ALU and 2nd branch)]
  tma_port_1
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 1 (ALU)]
  tma_port_5
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 5 ([SNB+] Branches and ALU; [HSW+] ALU)]
  tma_port_6
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port 6 ([HSW+]Primary Branch and simple ALU)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]

tma_issueBC: [Metrics related by the issue $issueBC]
  tma_info_bottleneck_big_code
       [Total pipeline cost of instruction fetch related bottlenecks by large
        code footprint programs (i-side cache; TLB and BTB misses)]
  tma_info_bottleneck_branching_overhead
       [Total pipeline cost of branch related instructions (used for program
        control-flow including function calls)]

tma_issueBM: [Metrics related by the issue $issueBM]
  tma_branch_mispredicts
       [This metric represents fraction of slots the CPU has wasted due to
        Branch Misprediction]
  tma_info_bad_spec_branch_misprediction_cost
       [Branch Misprediction Cost: Fraction of TMA slots wasted per
        non-speculative branch misprediction (retired JEClear)]
  tma_info_bottleneck_mispredictions
       [Total pipeline cost of Branch Misprediction related bottlenecks]
  tma_mispredicts_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Branch Misprediction at execution stage]

tma_issueBW: [Metrics related by the issue $issueBW]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_info_bottleneck_memory_bandwidth
       [Total pipeline cost of (external) Memory Bandwidth related bottlenecks]
  tma_info_system_dram_bw_use
       [Average external Memory Bandwidth Use for reads and writes [GB / sec]]
  tma_mem_bandwidth
       [This metric estimates fraction of cycles where the core's performance
        was likely hurt due to approaching bandwidth limits of external memory
        (DRAM)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

tma_issueD0: [Metrics related by the issue $issueD0]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_few_uops_instructions
       [This metric represents fraction of slots where the CPU was retiring
        instructions that that are decoder into two or up to ([SNB+] four;
        [ADL+] five) uops]

tma_issueFB: [Metrics related by the issue $issueFB]
  tma_dsb_switches
       [This metric represents fraction of cycles the CPU was stalled due to
        switches from DSB to MITE pipelines]
  tma_fetch_bandwidth
       [This metric represents fraction of slots the CPU was stalled due to
        Frontend bandwidth issues]
  tma_info_botlnk_l2_dsb_misses
       [Total pipeline cost of DSB (uop cache) misses - subset of the
        Instruction_Fetch_BW Bottleneck]
  tma_info_frontend_dsb_coverage
       [Fraction of Uops delivered by the DSB (aka Decoded ICache; or Uop
        Cache)]
  tma_info_inst_mix_iptb
       [Instruction per taken branch]
  tma_lcp
       [This metric represents fraction of cycles CPU was stalled due to
        Length Changing Prefixes (LCPs)]

tma_issueFL: [Metrics related by the issue $issueFL]
  tma_info_botlnk_l2_ic_misses
       [Total pipeline cost of Instruction Cache misses - subset of the
        Big_Code Bottleneck]

tma_issueL1: [Metrics related by the issue $issueL1]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]

tma_issueLat: [Metrics related by the issue $issueLat]
  tma_info_bottleneck_memory_latency
       [Total pipeline cost of Memory Latency related bottlenecks (external
        memory and off-core caches)]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_mem_latency
       [This metric estimates fraction of cycles where the performance was
        likely hurt due to latency from external memory (DRAM)]

tma_issueMC: [Metrics related by the issue $issueMC]
  tma_clears_resteers
       [This metric represents fraction of cycles the CPU was stalled due to
        Branch Resteers as a result of Machine Clears]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueMS: [Metrics related by the issue $issueMS]
  tma_microcode_sequencer
       [This metric represents fraction of slots the CPU was retiring uops
        fetched by the Microcode Sequencer (MS) unit]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueMV: [Metrics related by the issue $issueMV]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]

tma_issueRFO: [Metrics related by the issue $issueRFO]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_issueSL: [Metrics related by the issue $issueSL]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]

tma_issueSO: [Metrics related by the issue $issueSO]
  tma_ms_switches
       [This metric estimates the fraction of cycles when the CPU was stalled
        due to switches of uop delivery to the Microcode Sequencer (MS)]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

tma_issueSmSt: [Metrics related by the issue $issueSmSt]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

tma_issueSpSt: [Metrics related by the issue $issueSpSt]
  tma_split_stores
       [This metric represents rate of split store accesses]

tma_issueSyncxn: [Metrics related by the issue $issueSyncxn]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_machine_clears
       [This metric represents fraction of slots the CPU has wasted due to
        Machine Clears]

tma_issueTLB: [Metrics related by the issue $issueTLB]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_info_bottleneck_memory_data_tlbs
       [Total pipeline cost of Memory Address Translation related bottlenecks
        (data-side TLBs)]

tma_l1_bound_group: [Metrics contributing to tma_l1_bound category]
  tma_4k_aliasing
       [This metric estimates how often memory load accesses were aliased by
        preceding stores (in program order) with a 4K address offset]
  tma_dtlb_load
       [This metric roughly estimates the fraction of cycles where the Data
        TLB (DTLB) was missed by load accesses]
  tma_fb_full
       [This metric does a *rough estimation* of how often L1D Fill Buffer
        unavailability limited additional L1D miss memory access requests to
        proceed]
  tma_lock_latency
       [This metric represents fraction of cycles the CPU spent handling cache
        misses due to lock operations]
  tma_split_loads
       [This metric estimates fraction of cycles handling memory load split
        accesses - load that cross 64-byte cache line boundary]
  tma_store_fwd_blk
       [This metric roughly estimates fraction of cycles when the memory
        subsystem had loads blocked since they could not forward data from
        earlier (in program order) overlapping stores]

tma_l3_bound_group: [Metrics contributing to tma_l3_bound category]
  tma_contested_accesses
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to contested accesses]
  tma_data_sharing
       [This metric estimates fraction of cycles while the memory subsystem
        was handling synchronizations due to data-sharing accesses]
  tma_l3_hit_latency
       [This metric represents fraction of cycles with demand load accesses
        that hit the L3 cache under unloaded scenarios (possibly L3 latency
        limited)]
  tma_sq_full
       [This metric measures fraction of cycles where the Super Queue (SQ) was
        full taking into account all request-types and both hardware SMT
        threads (Logical Processors)]

tma_light_operations_group: [Metrics contributing to tma_light_operations category]
  tma_branch_instructions
       [This metric represents fraction of slots where the CPU was retiring
        branch instructions]
  tma_fp_arith
       [This metric represents overall arithmetic floating-point (FP)
        operations fraction the CPU has executed (retired)]
  tma_memory_operations
       [This metric represents fraction of slots where the CPU was retiring
        memory operations -- uops for memory load or store accesses]
  tma_nop_instructions
       [This metric represents fraction of slots where the CPU was retiring
        NOP (no op) instructions]
  tma_other_light_ops
       [This metric represents the remaining light uops fraction the CPU has
        executed - remaining means not covered by other sibling nodes]

tma_memory_bound_group: [Metrics contributing to tma_memory_bound category]
  tma_dram_bound
       [This metric estimates how often the CPU was stalled on accesses to
        external memory (DRAM) by loads]
  tma_l1_bound
       [This metric estimates how often the CPU was stalled without loads
        missing the L1 data cache]
  tma_l2_bound
       [This metric estimates how often the CPU was stalled due to L2 cache
        accesses by loads]
  tma_l3_bound
       [This metric estimates how often the CPU was stalled due to loads
        accesses to L3 cache or contended with a sibling Core]
  tma_store_bound
       [This metric estimates how often CPU was stalled due to RFO store
        memory accesses; RFO store issue a read-for-ownership request before
        the write]

tma_microcode_sequencer_group: [Metrics contributing to tma_microcode_sequencer category]
  tma_assists
       [This metric estimates fraction of slots the CPU retired uops delivered
        by the Microcode_Sequencer as a result of Assists]
  tma_cisc
       [This metric estimates fraction of cycles the CPU retired uops
        originated from CISC (complex instruction set computer) instruction]

tma_mite_group: [Metrics contributing to tma_mite category]
  tma_decoder0_alone
       [This metric represents fraction of cycles where decoder-0 was the only
        active decoder]
  tma_mite_4wide
       [This metric represents fraction of cycles where (only) 4 uops were
        delivered by the MITE pipeline]

tma_ports_utilization_group: [Metrics contributing to tma_ports_utilization category]
  tma_ports_utilized_0
       [This metric represents fraction of cycles CPU executed no uops on any
        execution port (Logical Processor cycles since ICL, Physical Core
        cycles otherwise)]
  tma_ports_utilized_1
       [This metric represents fraction of cycles where the CPU executed total
        of 1 uop per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]
  tma_ports_utilized_2
       [This metric represents fraction of cycles CPU executed total of 2 uops
        per cycle on all execution ports (Logical Processor cycles since ICL,
        Physical Core cycles otherwise)]
  tma_ports_utilized_3m
       [This metric represents fraction of cycles CPU executed total of 3 or
        more uops per cycle on all execution ports (Logical Processor cycles
        since ICL, Physical Core cycles otherwise)]

tma_ports_utilized_0_group: [Metrics contributing to tma_ports_utilized_0 category]
  tma_mixing_vectors
       [The Mixing_Vectors metric gives the percentage of injected blend uops
        out of all uops issued]
  tma_serializing_operation
       [This metric represents fraction of cycles the CPU issue-pipeline was
        stalled due to serializing operations]

tma_ports_utilized_3m_group: [Metrics contributing to tma_ports_utilized_3m category]
  tma_alu_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution ports for ALU operations]
  tma_load_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Load operations]
  tma_store_op_utilization
       [This metric represents Core fraction of cycles CPU dispatched uops on
        execution port for Store operations]

tma_retiring_group: [Metrics contributing to tma_retiring category]
  tma_heavy_operations
       [This metric represents fraction of slots where the CPU was retiring
        heavy-weight operations -- instructions that require two or more uops
        or micro-coded sequences]
  tma_light_operations
       [This metric represents fraction of slots where the CPU was retiring
        light-weight operations -- instructions that require no more than one
        uop (micro-operation)]

tma_serializing_operation_group: [Metrics contributing to tma_serializing_operation category]
  tma_slow_pause
       [This metric represents fraction of cycles the CPU was stalled due to
        PAUSE Instructions]

tma_store_bound_group: [Metrics contributing to tma_store_bound category]
  tma_dtlb_store
       [This metric roughly estimates the fraction of cycles spent handling
        first-level data TLB store misses]
  tma_false_sharing
       [This metric roughly estimates how often CPU was handling
        synchronizations due to False Sharing]
  tma_split_stores
       [This metric represents rate of split store accesses]
  tma_store_latency
       [This metric estimates fraction of cycles the CPU spent handling L1D
        store misses]
  tma_streaming_stores
       [This metric estimates how often CPU was stalled due to Streaming store
        memory accesses; Streaming store optimize out a read request required
        by RFO stores]

transaction:
  tsx_aborted_cycles
       [Percentage of cycles in aborted transactions]
  tsx_cycles_per_elision
       [Number of cycles within a transaction divided by the number of
        elisions]
  tsx_cycles_per_transaction
       [Number of cycles within a transaction divided by the number of
        transactions]
  tsx_transactional_cycles
       [Percentage of cycles within a transaction region]
